{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pidipidi/cs470_IAI/blob/main/assignment_2/CNN_problem_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSuVSNspPs4f"
      },
      "source": [
        "# Problem 2: Convolutional Neural Networks (CNN)\n",
        "\n",
        "In this assignment, you will develop a neural network with convolution and pooling layers to perform image classification, and test it out on the [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries including PyTorch"
      ],
      "metadata": {
        "id": "FrzQeXCOCl1N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rq22uftGvsR4"
      },
      "outputs": [],
      "source": [
        "#Importing all libraries\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from torchvision import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wWxsXO7PyfT"
      },
      "source": [
        "### Download the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without data augmentation"
      ],
      "metadata": {
        "id": "KMFMeco2BO6G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITDbKtgGfcPa"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_transform = transforms.Compose(\n",
        "    [#transforms.RandomHorizontalFlip(p=0.5),\n",
        "     #transforms.RandomAffine(degrees=(-5, 5), translate=(0.1, 0.1), scale=(0.9, 1.1), resample=PIL.Image.BILINEAR), ##DATA AUGMENTATION \n",
        "     transforms.ToTensor(),\n",
        "     #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "     ])\n",
        "\n",
        "test_transform = transforms.Compose(\n",
        "    [\n",
        "     transforms.ToTensor(),\n",
        "     #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "     ])\n",
        "\n",
        "dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=train_transform)\n",
        "train_set, val_set = torch.utils.data.random_split(dataset, [50000, 10000])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=8)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=128, shuffle=False, num_workers=8)\n",
        "\n",
        "test_set = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=test_transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False, num_workers=8)\n",
        "\n",
        "classes = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_2DjMXgzLGz"
      },
      "source": [
        "### Visualize 10 different classes of images in the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-afxRYXEffJx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# visualize training image for each class\n",
        "sample_images = [dataset.data[np.asarray(dataset.targets) == label][0] for label in range(10)]\n",
        "# show images\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
        "i = 0\n",
        "for row in axes:\n",
        "  for axis in row:\n",
        "    axis.set_xticks([])\n",
        "    axis.set_yticks([])\n",
        "    axis.set_xlabel(classes[i], fontsize=15)\n",
        "    axis.imshow(sample_images[i])\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyEoUgQH83AC"
      },
      "source": [
        "## 2.1. A CNN with MaxPooling layers\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Design the model \n",
        "You will implement a CNN model. In PyTorch, there are built-in functions that carry out the convolution steps for you. The following shows the key functions required for the design.\n",
        "\n",
        "\n",
        "*   nn.**[Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)**(in_channels, out_channels, kernel_size, stride=1, padding=0, bias=False): Convolution layer.\n",
        "*   nn.**[MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)**(kernel_size, stride=None, padding=0): Max pooling layer. \n",
        "* nn.**[Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)**(p=0.5, inplace=False): randomly zeroes some of the elements of the input tensor during training.\n",
        "*   F.**[relu](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)**(Z1): computes the elementwise ReLU of Z1 (which can be any shape).  \n",
        "*   x.**[view](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html)**(new_shape): returns a new tensor with the same data but different size. It is the equivalent of numpy function reshape (Gives a new shape to an array without changing its data). \n",
        "*   nn.**[Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)**(in_features, out_features): applies a linear transformation to the incoming data. It is also called a fully connected layer. "
      ],
      "metadata": {
        "id": "eQF2wj0nDSnF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzWwNSgD6PjM"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Problem 2: Implementing your own CNN\n",
        "# a. Convolution and MaxPooling layers\n",
        "\n",
        "\n",
        "class CNN_Max(nn.Module):\n",
        "  \"\"\"\n",
        "  A convolutional neural network (CNN). In this CNN object, we will use following\n",
        "  dimensions:\n",
        "\n",
        "  input_size: the dimension d of the input data.                        \n",
        "  hidden_size: the number of neurons h in the hidden layer.             \n",
        "  output_size: the number of classes c, which is 10 in our task          \n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    An initialization function. This object of network is a simple feed-forward \n",
        "    network. It takes an input to pass to muitiple layers. Then, provide the \n",
        "    output. The layers are initialized after their creation. \n",
        "\n",
        "    In this problem, we will use following set of parameters building a CNN/\n",
        "\n",
        "    conv: convolutional kernel size, which is 3 by 3 with bias                         \n",
        "    pool: pooling kernel-size, which is 2 by 2    \n",
        "    dropout: random zeroing layer with probability 0.4                            \n",
        "    fc: fully-connected layer which uses affine operation y=Wx+b              \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    N/A\n",
        "    \"\"\"\n",
        "    super(CNN_Max, self).__init__()\n",
        "\n",
        "    #############################################################################\n",
        "    # PLACE YOUR CODE HERE                                                      #\n",
        "    ############################################################################# \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #############################################################################\n",
        "   \n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    A forward pass function. Using the pre-defined network modules, we can here \n",
        "    build a model designing its structure. \n",
        "\n",
        "    Parameters\n",
        "    ---------- \n",
        "    x: matrix  \n",
        "      an input data of shape (1, d, d), where d is the dimension of the input \n",
        "      image. \n",
        "  \n",
        "    Returns\n",
        "    ---------- \n",
        "    out:     \n",
        "      an output data given x.\n",
        "\n",
        "    \"\"\"\n",
        "    #############################################################################\n",
        "    # PLACE YOUR CODE HERE                                                      #\n",
        "    ############################################################################# \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #############################################################################\n",
        "\n",
        "    return out\n",
        "\n",
        "# create a CNN object\n",
        "net = CNN_Max()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Device:', device)\n",
        "net.to(device)\n",
        "\n",
        "num_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
        "print(\"Number of trainable parameters:\", num_params)\n",
        "\n",
        "from torchsummary import summary\n",
        "summary(net,(1,28,28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDoOISiD-j-y"
      },
      "source": [
        "### Train the designed model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knNfturO7P8H"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "\"\"\"\n",
        "\n",
        " You have to define the loss, for that please use cross entropy loss      \n",
        " Also, you must implement optimizer called Adam.                           \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#############################################################################\n",
        "# PLACE YOUR CODE HERE                                                      #\n",
        "############################################################################# \n",
        "\n",
        "\n",
        " \n",
        "#############################################################################\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
        "\n",
        "loss_hist, acc_hist = [], []\n",
        "loss_hist_val, acc_hist_val = [], []\n",
        "\n",
        "for epoch in range(30):\n",
        "  running_loss = 0.0\n",
        "  correct = 0\n",
        "  for data in train_loader:\n",
        "    batch, labels = data\n",
        "    batch, labels = batch.to(device), labels.to(device)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    First, set the gradients to zero. Then obtain predictions from your CNN   \n",
        "    model. After that, pass into loss to calculate the difference between the \n",
        "    prediction and labels. Next, you have to compute the gradients with       \n",
        "    respect to the tensor.  \n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    #############################################################################\n",
        "    # PLACE YOUR CODE HERE                                                      #\n",
        "    ############################################################################# \n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "    #############################################################################\n",
        "    optimizer.step()\n",
        "\n",
        "    # compute training statistics\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  avg_loss = running_loss / len(train_set)\n",
        "  avg_acc = correct / len(train_set)\n",
        "  loss_hist.append(avg_loss)\n",
        "  acc_hist.append(avg_acc)\n",
        "\n",
        "  # validation statistics\n",
        "  net.eval()\n",
        "  with torch.no_grad():\n",
        "    loss_val = 0.0\n",
        "    correct_val = 0\n",
        "    for data in val_loader:\n",
        "      batch, labels = data\n",
        "      batch, labels = batch.to(device), labels.to(device)\n",
        "      outputs = net(batch)\n",
        "      loss = criterion(outputs, labels)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      correct_val += (predicted == labels).sum().item()\n",
        "      loss_val += loss.item()\n",
        "    avg_loss_val = loss_val / len(val_set)\n",
        "    avg_acc_val = correct_val / len(val_set)\n",
        "    loss_hist_val.append(avg_loss_val)\n",
        "    acc_hist_val.append(avg_acc_val)\n",
        "  net.train()\n",
        "\n",
        "  scheduler.step(avg_loss_val)\n",
        "  print('[epoch %d] loss: %.5f accuracy: %.4f val loss: %.5f val accuracy: %.4f' % (epoch + 1, avg_loss, avg_acc, avg_loss_val, avg_acc_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the classification accuracies"
      ],
      "metadata": {
        "id": "_P5cyYuvF4Lk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vg2nMdpG7Vhx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "  You have to plot two graphs, one for loss of training and validation data \n",
        "  and second one for accuarcy of training and validation data.              \n",
        "  Set x-axis to number of epochs and y-axis to loss or accuracy. Set legend \n",
        "  equal to training and validation set.                                      \n",
        "\n",
        "\"\"\"\n",
        "legend = ['Train', 'Validation']\n",
        "#############################################################################\n",
        "# PLACE YOUR CODE HERE                                                      #\n",
        "############################################################################# \n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        " \n",
        "#############################################################################\n",
        "plt.legend(legend, loc='upper left')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save and download the learned weights"
      ],
      "metadata": {
        "id": "n87pyow5GZcV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwG1GQlpQDlK"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "torch.save(net.state_dict(), 'checkpoint.pth')\n",
        "# download checkpoint file\n",
        "files.download('checkpoint.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the learned model with the test dataset"
      ],
      "metadata": {
        "id": "EB8wGHWCGgxK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlrJrurZQMsy"
      },
      "outputs": [],
      "source": [
        "pred_vec = []\n",
        "correct = 0\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        batch, labels = data\n",
        "        batch, labels = batch.to(device), labels.to(device)\n",
        "        outputs = net(batch)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        pred_vec.append(predicted)\n",
        "    pred_vec = torch.cat(pred_vec)\n",
        "\n",
        "print('Accuracy on the 10000 test images: %.2f %%' % (100 * correct / len(test_set)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}