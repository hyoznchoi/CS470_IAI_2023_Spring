{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pidipidi/CS470_IAI_2023_Spring/blob/main/assignment_3/RL_assignment3_problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS470 Assignment 3\n",
        "In this assignment, we will first design a **Markov Decision Process (MDP)** for a *gridworld* environment. Then, on top of it, we implement and run **dynamic programming (DP)** approaches. Note that you must run this file on **Google Chrome**, otherwise you may not be able to play recorded videos."
      ],
      "metadata": {
        "id": "L96B5Ed06r8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements for initialization\n",
        "We will first install dependencies and declare auxiliary functions for visualization"
      ],
      "metadata": {
        "id": "tHYpkF-KbM1n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMLNlciUp9y1",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Run the cell below to install dependencies\n",
        "\n",
        "#Install dependencies to visualize agents \n",
        "!pip install pyglet==1.5.1  &> /dev/null\n",
        "!apt install -y python-opengl ffmpeg xvfb &> /dev/null\n",
        "!pip install pyvirtualdisplay &> /dev/null\n",
        "!pip install gymnasium &> /dev/null\n",
        "!pip install numpy &> /dev/null\n",
        "\n",
        "# !pip install huggingface_hub &> /dev/null\n",
        "!pip install pickle5 &> /dev/null\n",
        "!pip install pyyaml==6.0 &> /dev/null \n",
        "!pip install imageio imageio_ffmpeg &> /dev/null\n",
        "\n",
        "!apt-get install -y python x11-utils &> /dev/null\n",
        "!pip install scikit-video ffio pyrender &> /dev/null\n",
        "# !pip install tensorflow_probability==0.12.0 &> /dev/null\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import error, spaces, utils\n",
        "from gymnasium.utils import seeding\n",
        "from typing import List, Optional\n",
        "\n",
        "import imageio, random, copy\n",
        "import sys, time, os, base64, io\n",
        "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n",
        "\n",
        "import IPython, functools, matplotlib, cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "from PIL import Image as Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run the cell below to declare auxiliary functions\n",
        "\n",
        "from IPython.display import HTML\n",
        "def eval_policy(env, policy=None, num_episodes=10):\n",
        "    \"\"\"Evaluate a model (i.e., policy) running on the input environment\"\"\"\n",
        "    # env = gym.make(env_name)\n",
        "    obs, _ = env.reset()\n",
        "    prev_obs = obs\n",
        "    counter = 0\n",
        "    done = False\n",
        "    num_runs = 0\n",
        "    episode_reward = 0\n",
        "    episode_rewards = []\n",
        "    while num_runs < num_episodes:\n",
        "        if policy is not None:\n",
        "            action = policy(obs)\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "        \n",
        "        prev_obs = obs\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        counter += 1\n",
        "        episode_reward += reward\n",
        "        if done or truncated:\n",
        "            counter = 0\n",
        "            num_runs += 1\n",
        "            obs, _ = env.reset()\n",
        "            episode_rewards.append(episode_reward)\n",
        "            episode_reward = 0        \n",
        "    return episode_rewards\n",
        "\n",
        "def save_video_of_model(env_name, model=None, suffix=\"\", num_episodes=10):\n",
        "    \"\"\"\n",
        "    Record a video that shows the behavior of an agent following a model \n",
        "    (i.e., policy) on the input environment\n",
        "    \"\"\"\n",
        "    import skvideo.io\n",
        "    from pyvirtualdisplay import Display\n",
        "    display = Display(visible=0, size=(400, 300))\n",
        "    display.start()\n",
        "\n",
        "    env = gym.make(env_name)\n",
        "    obs, _ = env.reset()\n",
        "    prev_obs = obs\n",
        "\n",
        "    filename = env_name + suffix + \".mp4\"\n",
        "    output_video = skvideo.io.FFmpegWriter(filename)\n",
        "\n",
        "    counter = 0\n",
        "    done = False\n",
        "    num_runs = 0\n",
        "    returns = 0\n",
        "    while num_runs < num_episodes:\n",
        "        frame = env.render()\n",
        "        output_video.writeFrame(frame)\n",
        "\n",
        "        if \"Gridworld\" in env_name:\n",
        "            input_obs = obs\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown env for saving: {env_name}\")\n",
        "\n",
        "        if model is not None:\n",
        "            action = model(input_obs)\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "\n",
        "        prev_obs = obs\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        counter += 1\n",
        "        returns += reward\n",
        "        if done or truncated:\n",
        "            num_runs += 1\n",
        "            obs, _ = env.reset()\n",
        "\n",
        "    output_video.close()\n",
        "    print(\"Successfully saved {} frames into {}!\".format(counter, filename))\n",
        "    return filename, returns / num_runs\n",
        "\n",
        "def play_video(filename, width=None):\n",
        "    \"\"\"Play the input video\"\"\"\n",
        "\n",
        "    from base64 import b64encode\n",
        "    mp4 = open(filename,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    html = \"\"\"\n",
        "    <video width=400 controls>\n",
        "          <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % data_url\n",
        "    return  html\n",
        "\n",
        "\n",
        "def render_value_map_with_action(env, Q, policy=None):\n",
        "    '''\n",
        "    Render a state (or action) value grid map.\n",
        "    V[s] = max(Q[s,a])\n",
        "    '''\n",
        "    Q = Q.copy()\n",
        "    from matplotlib.colors import LinearSegmentedColormap\n",
        "    n = env.grid_map_shape[0]\n",
        "    m = env.grid_map_shape[1]\n",
        "    \n",
        "    if len(np.shape(Q))>1:\n",
        "        V = np.amax(Q, axis=1) \n",
        "        V = V.reshape((m,n)).T\n",
        "    else:\n",
        "        V = Q.reshape((m,n)).T\n",
        "\n",
        "    import itertools\n",
        "    symbol = ['.', '^','v', '<', '>']\n",
        "    x = range(0, env.grid_map_shape[0])\n",
        "    y = range(0, env.grid_map_shape[1])\n",
        "\n",
        "    min_val = V[0,0]\n",
        "    obstacles = np.zeros([n,m])\n",
        "    for obstacle in env.obstacles:\n",
        "        posx = obstacle % env.grid_map_shape[0]\n",
        "        posy = obstacle // env.grid_map_shape[0]\n",
        "        V[posx, posy] = min_val\n",
        "        obstacles[posx, posy] = 1\n",
        "\n",
        "    plt.imshow(V, cmap='jet', interpolation='nearest')\n",
        "    for s in range(env.observation_space.n):\n",
        "        twod_state = env.serial_to_twod(s)\n",
        "        state_inds = s\n",
        "        best_action = policy(s)\n",
        "        plt.plot([twod_state[1]], [twod_state[0]], marker=symbol[best_action], linestyle='none', color='k')\n",
        "\n",
        "    dark_low = ((0., 1., 1.),\n",
        "            (.3, 1., 0.),\n",
        "            (1., 0., 0.))\n",
        "            \n",
        "    cdict = {'red':  dark_low,\n",
        "        'green': dark_low,\n",
        "        'blue': dark_low}\n",
        "\n",
        "    cdict3 = {'red':  dark_low,\n",
        "        'green': dark_low,\n",
        "        'blue': dark_low,\n",
        "        'alpha': ((0.0, 0.0, 0.0),\n",
        "                  (0.3, 0.0, 1.0),\n",
        "                  (1.0, 1.0, 1.0))\n",
        "        }\n",
        "    dropout_high = LinearSegmentedColormap('Dropout', cdict3)\n",
        "    plt.imshow(obstacles, cmap = dropout_high)\n",
        "    plt.show()\n",
        "\n",
        "def collect_traj(env, policy=None, num_episodes=10):\n",
        "    \"\"\"Collect trajectories (rollouts) following the input policy\"\"\"\n",
        "    obs, _ = env.reset()\n",
        "    prev_obs = obs\n",
        "    done = False\n",
        "    num_runs = 0\n",
        "    episode_rewards = []\n",
        "    episode_reward = 0\n",
        "    traj = []\n",
        "    trajs = []\n",
        "\n",
        "    while num_runs < num_episodes:\n",
        "        input_obs = obs\n",
        "        if policy is not None:\n",
        "            action = policy(input_obs)\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "        prev_obs = obs\n",
        "        traj.append(obs)\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        episode_reward += reward\n",
        "        if done or truncated:\n",
        "            num_runs += 1\n",
        "            traj.append(obs)\n",
        "            trajs.append(traj)\n",
        "\n",
        "            traj = []\n",
        "            obs, _ = env.reset()\n",
        "\n",
        "            episode_rewards.append(episode_reward)\n",
        "            episode_reward = 0\n",
        "    return trajs#, episode_rewards\n",
        "\n",
        "def plot_trajs(env, trajectories):\n",
        "    \"\"\"Plot the input trajectories\"\"\"\n",
        "    from matplotlib.colors import LinearSegmentedColormap\n",
        "    n = env.grid_map_shape[0]\n",
        "    m = env.grid_map_shape[1]\n",
        "    V = np.zeros([n,m])\n",
        "    obstacles = np.zeros([n,m])\n",
        "    for obstacle in env.obstacles:\n",
        "        posx = obstacle % env.grid_map_shape[0]\n",
        "        posy = obstacle // env.grid_map_shape[0]\n",
        "        obstacles[posx, posy] = 1\n",
        "    dark_low = ((0., 1., 1.),\n",
        "            (.3, 1., 0.),\n",
        "            (1., 0., 0.))\n",
        "    cdict = {'red':  dark_low,\n",
        "        'green': dark_low,\n",
        "        'blue': dark_low}\n",
        "    cdict3 = {'red':  dark_low,\n",
        "        'green': dark_low,\n",
        "        'blue': dark_low,\n",
        "        'alpha': ((0.0, 0.0, 0.0),\n",
        "                  (0.3, 0.0, 1.0),\n",
        "                  (1.0, 1.0, 1.0))\n",
        "        }\n",
        "    dropout_high = LinearSegmentedColormap('Dropout', cdict3)\n",
        "    plt.imshow(obstacles, cmap = dropout_high)\n",
        "    for trajectory in trajectories:\n",
        "        traj_2d = np.array([ env.serial_to_twod(s) for s in trajectory ])\n",
        "        x = traj_2d[:, 0]\n",
        "        y = traj_2d[:, 1]\n",
        "        plt.plot(y, x, alpha=0.1, color='r')\n",
        "    #plt.gca().invert_yaxis()\n",
        "    plt.show()\n",
        "\n",
        "def plot_grid(env):\n",
        "    \"\"\"Plot the input trajectories\"\"\"\n",
        "    from matplotlib.colors import LinearSegmentedColormap\n",
        "    n = env.grid_map_shape[0]\n",
        "    m = env.grid_map_shape[1]\n",
        "    V = np.zeros([n,m])\n",
        "    grid_map = np.zeros([n,m])\n",
        "    # for obstacles\n",
        "    for obstacle in env.obstacles:\n",
        "        posx = obstacle % env.grid_map_shape[0]\n",
        "        posy = obstacle // env.grid_map_shape[0]\n",
        "        grid_map[posx, posy] = 1\n",
        "    # for trap\n",
        "    for trap in env.traps:\n",
        "        posx = trap % env.grid_map_shape[0]\n",
        "        posy = trap // env.grid_map_shape[0]\n",
        "        grid_map[posx, posy] = -1\n",
        "\n",
        "    # create discrete colormap\n",
        "    cmap = colors.ListedColormap(['red', 'white', 'black'])\n",
        "    bounds = [-1.5,-0.5, 0.5,1.5]\n",
        "    norm = colors.BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "    plt.imshow(grid_map, cmap = cmap, norm=norm)\n",
        "    #plt.pcolormesh(grid_map, edgecolors='k', linewidth=2, cmap = cmap, norm=norm)\n",
        "\n",
        "    x1 = range(env.grid_map_shape[0])\n",
        "    y1 = range(env.grid_map_shape[1])\n",
        "\n",
        "    for i in range(env.grid_map_shape[0]):\n",
        "        for j in range(env.grid_map_shape[1]):\n",
        "            binvalue = int(j*env.grid_map_shape[0]+i)\n",
        "            plt.text(y1[j] + 0.0, \n",
        "                    x1[i] + 0.0, \n",
        "                    binvalue,\n",
        "                    color='white' if binvalue in env.obstacles else 'black',\n",
        "                    ha='center', va='center', size=8)\n",
        "\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "MfKdFRBd30bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1: Markov Decision Process\n",
        "In this problem, you design an MDP for a stochastic version of *grid-world* environment. You will implement *transition_model*, *compute_reward*, *is_done* and *step* functions following the rules defined in the assignment PDF."
      ],
      "metadata": {
        "id": "Moxb_9vubXU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseGridEnv(gym.Env):\n",
        "    metadata = {'render_modes': ['human', 'rgb_array'],\n",
        "                \"render_fps\": 4}\n",
        "    def __init__(self, render_mode='rgb_array', size=[8,10], start=None,\n",
        "                 epsilon=0.2, obstacle=None, trap=None):\n",
        "        \"\"\"\n",
        "        An initialization function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        size: a list of integers\n",
        "            the dimension of 2D grid environment\n",
        "        start: integer\n",
        "            start state (i.e., location)\n",
        "        epsilon: float\n",
        "            the probability of taking random actions\n",
        "        obstacle: \n",
        "\n",
        "        \"\"\"\n",
        "        self.render_mode = render_mode\n",
        "        self.grid_map_shape = [size[0], size[1]]  # The size of the map\n",
        "        self.epsilon = epsilon  # action-failure probability\n",
        "        self.obstacles = obstacle   # list of obstacle positions\n",
        "        self.traps = trap # list of trap positions\n",
        "        \n",
        "        ''' set observation space and action space '''\n",
        "        self.observation_space = spaces.Discrete( size[0] * size[1])\n",
        "        self.action_space = spaces.Discrete( 5 )\n",
        "    \n",
        "        self.start_state = 0\n",
        "        self.terminal_state = size[0] * size[1] - 1\n",
        "\n",
        "    def serial_to_twod(self, ind):\n",
        "        \"\"\"Convert a serialized state number to a 2D map's state coordinate\"\"\"\n",
        "        return np.array( [ ind % self.grid_map_shape[0], ind // self.grid_map_shape[0]])\n",
        "\n",
        "    def twod_to_serial(self, twod):\n",
        "        \"\"\"Convert a 2D map's state coordinate to a serialized state number\"\"\"\n",
        "        return np.array( twod[1]* self.grid_map_shape[0] + twod[0])\n",
        "\n",
        "    def reset(self, \n",
        "              *,\n",
        "              seed: Optional[int] = None,\n",
        "              options: Optional[dict] = None,):\n",
        "        \"\"\"Rest the environment by initializaing the start state \"\"\"\n",
        "        super().reset(seed=seed)\n",
        "        self.observation = self.start_state\n",
        "        return self.observation, {'prob': 1}\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Render the agent state\"\"\"\n",
        "        pixel_size = 20\n",
        "        img = np.zeros([ pixel_size * self.grid_map_shape[0], pixel_size * self.grid_map_shape[1],3], dtype=np.uint8)\n",
        "        for obstacle in self.obstacles:\n",
        "          pos_x, pos_y = self.serial_to_twod(obstacle)\n",
        "          img[pixel_size*pos_x: pixel_size*(1+pos_x), pixel_size*pos_y: pixel_size*(1+pos_y)] += np.array([255,0,0], dtype=np.uint8)\n",
        "        agent_state = self.serial_to_twod(self.observation)   \n",
        "        agent_target_state = self.serial_to_twod(self.terminal_state)\n",
        "        img[pixel_size*agent_state[0]: pixel_size*(1+agent_state[0]), pixel_size*agent_state[1]: pixel_size*(1+agent_state[1])] += np.array([0,0,255], dtype=np.uint8)\n",
        "        img[pixel_size*agent_target_state[0]: pixel_size*(1+agent_target_state[0]), pixel_size*agent_target_state[1]: pixel_size*(1+agent_target_state[1])] += np.array([0,255,0], dtype=np.uint8)\n",
        "        if self.render_mode == 'human':\n",
        "          fig = plt.figure(0)\n",
        "          plt.clf()\n",
        "          plt.imshow(img, cmap='gray')\n",
        "          fig.canvas.draw()\n",
        "          plt.pause(0.01)\n",
        "        if self.render_mode == 'rgb_array':\n",
        "          return img\n",
        "        return \n",
        "\n",
        "    def _close_env(self):\n",
        "        \"\"\"Close the environment screen\"\"\"\n",
        "        plt.close(1)\n",
        "        return\n"
      ],
      "metadata": {
        "id": "lGUcYuZ06x15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the target grid environment for this assignment."
      ],
      "metadata": {
        "id": "pJIVD7nS3snZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ed3bkDS5Q3K"
      },
      "outputs": [],
      "source": [
        "class GridEnv(BaseGridEnv):\n",
        "    \"\"\"\n",
        "    A grid-world environment.\n",
        "    \"\"\"\n",
        "    def transition_model(self, state, action):\n",
        "        \"\"\"\n",
        "        A transition model that return a list of probabilities of transitions\n",
        "        to next states when the agent select 'action' at the 'state': T(s' | s,a)\n",
        "\n",
        "        In our envrionemnt, if the state is in traps or in a goal, \n",
        "        it will stay in its state at any action\n",
        "\n",
        "        Parameters\n",
        "        ----------        \n",
        "        state: integer\n",
        "            a serialized state index\n",
        "        action: integer\n",
        "            action index        \n",
        "\n",
        "        Returns\n",
        "        -------  \n",
        "        probs: numpy array with a length of {size of state space}\n",
        "            probabilities of transition to the next_state ...                    \n",
        "        \"\"\"\n",
        "        if not isinstance(state, int):\n",
        "            state = state.item()\n",
        "\n",
        "        # the transition probabilities to the next states\n",
        "        probs = np.zeros(self.observation_space.n)\n",
        "\n",
        "        # Left top is [0,0], \n",
        "        action_pos_dict = {0: [0,0], 1:[-1, 0], 2:[1,0], 3:[0,-1], 4:[0,1]}\n",
        "\n",
        "        ###################################################################\n",
        "        #####################   PLACE YOUR CODE HERE   ####################\n",
        "        ###                                                             ###\n",
        "        ### Instruction                                                 ###\n",
        "        ### -----------                                                 ###\n",
        "        ### - You have to fill out the \"probs\" variable following the   ###\n",
        "        ###   the assignment-problem 1's transition model rules         ###        \n",
        "        ### - Hint: you have to find the next state based on the        ###\n",
        "        ###         available action at the current state               ###\n",
        "        ### - Hint: you need to use serial_to_twod & twod_to_serial     ###\n",
        "        ###         functions                                           ###        \n",
        "        ###\n",
        "        ### Example\n",
        "        ### --------\n",
        "        ### After some initializations...\n",
        "        ### for action in availabe actions:\n",
        "        ###     find a next state\n",
        "        ###     fill out probs[next state] \n",
        "        ###        \n",
        "\n",
        "\n",
        "        # initialization or handling exceptions\n",
        "        # ....\n",
        "               \n",
        "        #for new_action, prob in enumerate(action_probs):\n",
        "            # ...\n",
        "\n",
        "            # probs[next_state] ... \n",
        "        ###                                                             ###\n",
        "        ###                                                             ###\n",
        "        ###################################################################\n",
        "        ###################################################################\n",
        "\n",
        "        return probs\n",
        "\n",
        "\n",
        "    def compute_reward(self, state, action, next_state):\n",
        "        \"\"\"\n",
        "        A reward function that returns the total reward after selecting 'action'\n",
        "        at the 'state'. In this environment, \n",
        "        (a) If it reaches a goal state, it terminates returning a reward of +5\n",
        "        (b) If it reaches a trap, it terminates returning a penalty of -10\n",
        "        (c) For any action, it add a step penalty of -0.1\n",
        "\n",
        "        Parameters\n",
        "        ----------        \n",
        "        state: integer\n",
        "            a serialized state index\n",
        "        action: integer\n",
        "            action index         \n",
        "        next_state: integer\n",
        "            a serialized state index\n",
        "\n",
        "        Returns\n",
        "        -------  \n",
        "        reward: float\n",
        "            a total reward value\n",
        "        \"\"\"\n",
        "\n",
        "        reward = 0\n",
        "        ###################################################################\n",
        "        #####################   PLACE YOUR CODE HERE   ####################\n",
        "        ###                                                             ###\n",
        "        ### Instruction                                                 ###\n",
        "        ### -----------                                                 ###\n",
        "        ### - fill out the reward variable                              ###\n",
        "        ###                                                             ###\n",
        "\n",
        "        \n",
        "        #reward ...\n",
        "        \n",
        "        ###                                                             ###\n",
        "        ###                                                             ###\n",
        "        ###################################################################\n",
        "        ###################################################################\n",
        "        return reward\n",
        "    \n",
        "    def is_done(self, state, action, next_state):\n",
        "        \"\"\"\n",
        "        Return True when the agent is in a terminal state or a trap, \n",
        "        otherwise return False\n",
        "\n",
        "        Parameters\n",
        "        ----------        \n",
        "        state: integer\n",
        "            a serialized state index\n",
        "        action: integer\n",
        "            action index         \n",
        "        next_state: integer\n",
        "            a serialized state index\n",
        "\n",
        "        Returns\n",
        "        -------  \n",
        "        done: Bool\n",
        "            the result of termination or collision\n",
        "        \"\"\"\n",
        "        done = None\n",
        "\n",
        "        ###################################################################\n",
        "        #####################   PLACE YOUR CODE HERE   ####################\n",
        "        ###                                                             ###\n",
        "        ### Instruction                                                 ###\n",
        "        ### -----------                                                 ###\n",
        "        ### - fill out the \"done\" variable                              ###\n",
        "        \n",
        "        #done ... \n",
        "\n",
        "        ###                                                             ###\n",
        "        ###################################################################\n",
        "        ###################################################################\n",
        "\n",
        "        return done \n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        A step function that applies the input action to the environment.\n",
        "\n",
        "        Parameters\n",
        "        ----------        \n",
        "        action: integer\n",
        "            action index         \n",
        "\n",
        "        Returns\n",
        "        -------  \n",
        "        observation: integer\n",
        "            the outcome of the given action (i.e., next state)... s' ~ T(s'|s,a)\n",
        "        reward: float\n",
        "            the reward that would get for ... r(s, a, s')\n",
        "        done: Bool\n",
        "            the result signal of termination or collision\n",
        "        truncated: Bool\n",
        "            A boolean of whether a truncation condition is satisfied or not\n",
        "            (Do not need to implement.)\n",
        "        info: Dictionary\n",
        "            Information dictionary containing miscellaneous information...\n",
        "            such as the probability 'p' of the next state\n",
        "            (Do not need to implement info)\n",
        "\n",
        "        \"\"\"\n",
        "        done = False\n",
        "        truncated = False\n",
        "        action = int(action)\n",
        "        \n",
        "        probs = self.transition_model(self.observation, action)\n",
        "\n",
        "        ###################################################################\n",
        "        #####################   PLACE YOUR CODE HERE   ####################\n",
        "        ###                                                             ###\n",
        "        ### Instruction                                                 ###\n",
        "        ### -----------                                                 ###\n",
        "        ### - sample the next state considring the transition model     ###\n",
        "        ### - then, compute \"reward\" and \"done\"                         ###\n",
        "        ###   next_state = ...                                          ###\n",
        "        ###\n",
        "        ### Example                                                     ###\n",
        "        ### -----------                                                 ###\n",
        "        ### next state = ?\n",
        "        ### p = ?\n",
        "        ### self.observation = ?\n",
        "        ### reward = self.compute_reward(?, action, self.observation)   ###\n",
        "        ### done = self.is_done(?, action, self.observation)            ###\n",
        "\n",
        "        \n",
        "        #p = \n",
        "        \n",
        "        \n",
        "        #self.observation = \n",
        "        #reward = \n",
        "        #done = \n",
        "        \n",
        "        ###                                                             ###\n",
        "        ###################################################################\n",
        "        ###################################################################\n",
        "\n",
        "        return (self.observation, reward, done, truncated, {\"prob\": p})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Register the implemented grid environment in the OpenAI Gym."
      ],
      "metadata": {
        "id": "gXW10QEK35I8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnasium.envs.registration import register\n",
        "\n",
        "#if 'Gridworld-v0' in gym.envs.registration.registry.env_specs:\n",
        "#    del gym.envs.registration.registry.env_specs['Gridworld-v0']\n",
        "register(\n",
        "    id='Gridworld-v0',\n",
        "    entry_point=GridEnv,\n",
        "    max_episode_steps=30,\n",
        "    reward_threshold=100,\n",
        "    kwargs={'epsilon':0.05, 'size':[8, 10], \n",
        "            'obstacle':[15,18,26,27,37,47,50,51,59,66,54],\n",
        "            'trap':[34,35,36,77]}  \n",
        ")\n",
        "env = gym.make(\"Gridworld-v0\")\n",
        "\n",
        "# plot the environment\n",
        "plot_grid(env)\n"
      ],
      "metadata": {
        "id": "R0q7RNq_33Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transition model / dynamics / Environmental Interactions\n",
        "\n",
        "Following the problem description on the assignment PDF, implement the ***transition_model***, ***compute_reward***, ***is_done***, and ***step*** functions. Here, \n",
        "\n",
        "*   ***is_done*** function determines wether the given next_state terminates the episode or not. \n",
        "*   ***step*** function updates its *self.observation* to the next state.\n",
        "\n",
        "After implementation, please run the cell below to see the histogram of returns."
      ],
      "metadata": {
        "id": "-Vr04-7W-7Fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dummy_policy(state):\n",
        "    \"\"\"A dummy random policy\"\"\"\n",
        "    return np.random.choice([1,2,3,4], 1).item()\n",
        "\n",
        "# Plot the distribution of the collected trajectories\n",
        "trajs = collect_traj(env, policy=dummy_policy, num_episodes=100)\n",
        "plot_trajs(env, trajs)\n",
        "\n",
        "# Plot the histogram of returns after running a dummy policy\n",
        "returns = eval_policy(env, policy=dummy_policy, num_episodes=100)\n",
        "plt.hist(returns, bins=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SNr6dtyhA9WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add your code to print out the transition model at a specific coordinate with an action you want. \\\\\n",
        "1) print out the transition probabilities to all the next states given a current state $[3,1]$ and a selected action \"Down\":"
      ],
      "metadata": {
        "id": "dzvyKTbT0rP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################################\n",
        "#####################   PLACE YOUR CODE HERE   ####################\n",
        "###                                                             ###\n",
        "\n",
        "#p = env.transition_model(                 )\n",
        "###################################################################\n",
        "print(p.reshape(env.grid_map_shape[::-1]).T)"
      ],
      "metadata": {
        "id": "NB1LsQ670pQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Print out the transition probabilities to all the next states given a current state [0,6] and a selected action \"Right\":"
      ],
      "metadata": {
        "id": "U3mOHNAW3_2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################################\n",
        "#####################   PLACE YOUR CODE HERE   ####################\n",
        "###                                                             ###\n",
        "\n",
        "#p = env.transition_model(                 )\n",
        "\n",
        "###################################################################\n",
        "print(p.reshape(env.grid_map_shape[::-1]).T)"
      ],
      "metadata": {
        "id": "R4Y1eipc3qec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) print out the transition probabilities to all the next states given a current state $[3,5]$ and a selected action \"Right\":"
      ],
      "metadata": {
        "id": "q8a_3Dfl4FRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################################\n",
        "#####################   PLACE YOUR CODE HERE   ####################\n",
        "###                                                             ###\n",
        "\n",
        "#p = env.transition_model(                 )\n",
        "\n",
        "###################################################################\n",
        "print(p.reshape(env.grid_map_shape[::-1]).T)"
      ],
      "metadata": {
        "id": "9VfzdYSS1O8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2: Dynamic Programming (DP)"
      ],
      "metadata": {
        "id": "Q0Fj-ogmf-aM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! You can now run an agent to test your MDP implementation."
      ],
      "metadata": {
        "id": "O0Pj9gjjdq3n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ronn8mOIxeqr"
      },
      "outputs": [],
      "source": [
        "###################################################################\n",
        "##################   DO NOT MODIFY THIS CELL   ####################\n",
        "###                                                             ###\n",
        "\n",
        "# You can save a video of 10 episodes that visualizes the average of the returns \n",
        "# based on the current (dummy) policy.\n",
        "saved_run, returns = save_video_of_model(\"Gridworld-v0\", model=dummy_policy, num_episodes=10)\n",
        "HTML(play_video(saved_run))\n",
        "###################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (2.1) Value Iteration \n",
        "You implement and analyze the value iteration (VI) algorithm filling out ***value_iteration*** and ***get_action*** functions. Please, check the assignment PDF for more details. \n",
        "\n",
        "* Hint: you may need to implement your own ***arg_max*** function\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E6N2tAosd84l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-D-iWHBHT3X"
      },
      "outputs": [],
      "source": [
        "class ValueIteration:\n",
        "    \"\"\"\n",
        "    Value Iteration\n",
        "    \"\"\"\n",
        "    def __init__(self, env,  theta=0.0001, discount_factor=0.9):\n",
        "        \"\"\"\n",
        "        Initialize the VI class\n",
        "\n",
        "        Parameters\n",
        "        ----------        \n",
        "        env: object\n",
        "            an OpenAI Gym compatible environment\n",
        "        theta: float\n",
        "            a max error (termination) threshold \n",
        "        discount_factor: float\n",
        "            discount factor (i.e., gamma)\n",
        "\n",
        "        Returns\n",
        "        -------  \n",
        "        probs: numpy array with a length of {size of state space}\n",
        "            probabilities of transition to the next_state ...                    \n",
        "        \"\"\"      \n",
        "        self.env = env\n",
        "        self.V = np.zeros(env.observation_space.n)\n",
        "        self.discount_factor = discount_factor\n",
        "        self.theta= theta   # Maximum error thresold value\n",
        "    \n",
        "    def value_iteration(self):\n",
        "        \"\"\"\n",
        "        A value iteration function. Until the error bound reaches the threshold \n",
        "        (theta), The value table is updated by the Dynamic Programming \n",
        "        (refer to the lecture)\n",
        "        \"\"\"\n",
        "        errors = []\n",
        "        episode_rewards = []\n",
        "        while True:\n",
        "            max_error = 0\n",
        "            for state in range(env.observation_space.n):\n",
        "\n",
        "                Q = np.zeros(env.action_space.n)\n",
        "                ###############################################################\n",
        "                #####################   PLACE YOUR CODE HERE   ################\n",
        "                ###                                                         ###\n",
        "                ### Instruction                                             ###\n",
        "                ### -----------                                             ###\n",
        "                ### Fill out self.V by using the VI algorithm               ###      \n",
        "                ###\n",
        "                ### Example                                                 ###\n",
        "                ### -----------                                             ###\n",
        "                ### Initializations\n",
        "                ### For a in available actions\n",
        "                ###   For s' in available next states\n",
        "                ###     Compute reward(s,a,s'), etc\n",
        "                ###     Compute action value \n",
        "                ### Fill out \"self.V\" at the current state using the action values\n",
        "                \n",
        "                #for action\n",
        "\n",
        "\n",
        "                    #for next_state\n",
        "\n",
        "\n",
        "                        #Q[action] ....\n",
        "\n",
        "                \n",
        "                #max_error = \n",
        "                #self.V[state] = \n",
        "                ###                                                         ###\n",
        "                ###############################################################\n",
        "                ###############################################################\n",
        "            errors.append(max_error)\n",
        "            mean_ep_reward = np.mean( eval_policy(env, policy=self.get_action, num_episodes=10))\n",
        "            episode_rewards.append(mean_ep_reward)\n",
        "\n",
        "            if max_error < self.theta:\n",
        "                break\n",
        "        return episode_rewards, errors\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        Return the best action. \n",
        "        HINT: how do you handle if there are multiple actions with the highest value?\n",
        "\n",
        "        Parameters\n",
        "        ----------        \n",
        "        state: integer\n",
        "            a serialized state index\n",
        "\n",
        "        Returns\n",
        "        -------  \n",
        "        action: integer\n",
        "            an action index\n",
        "        \"\"\"\n",
        "\n",
        "        action = None\n",
        "        Q = np.zeros(env.action_space.n) # Q values\n",
        "\n",
        "        ###################################################################\n",
        "        #####################   PLACE YOUR CODE HERE   ####################\n",
        "        ###                                                             ###\n",
        "        ### Instruction                                                 ###\n",
        "        ### -----------                                                 ###\n",
        "        ### Find the best \"action\" that maximize Q value                ###      \n",
        "        ###                                                             ###  \n",
        "        ### Example                                                     ###\n",
        "        ### -----------                                                 ###\n",
        "        ### Compute Q values when you apply each action                 ###\n",
        "        ### For a in available actions\n",
        "        ###   For s' in next states\n",
        "        ###     Q[a] = ?  \n",
        "        ###\n",
        "        ### action = ?  \n",
        "             \n",
        "        \n",
        "        #for action\n",
        "            \n",
        "\n",
        "            #for next_state, prob\n",
        "\n",
        "\n",
        "                #Q[action] ...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #action = \n",
        "        ###                                                             ###\n",
        "        ###################################################################\n",
        "        ###################################################################\n",
        "        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After implementation, you can check your implementation with the following code."
      ],
      "metadata": {
        "id": "9-amthiWoG9e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tF8Y1jTj6sD2"
      },
      "outputs": [],
      "source": [
        "###################################################################\n",
        "##################   DO NOT MODIFY THIS CELL   ####################\n",
        "env = gym.make(\"Gridworld-v0\")\n",
        "vi = ValueIteration(env, discount_factor=0.9)\n",
        "ep_rews, errors = vi.value_iteration()\n",
        "###################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can check the state values of the first 8 states:"
      ],
      "metadata": {
        "id": "XPrDvLDG8nPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "5# the values of the first 10 states of the gridworld environment\n",
        "print(vi.V[:10])"
      ],
      "metadata": {
        "id": "qAHA2FXV8zon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also overlay the best action at each state and plot the distribution of trajectories produced by the trained VI policy:"
      ],
      "metadata": {
        "id": "B9FLE-H9851Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Visualize the policy with the value map and trajectories plot\n",
        "saved_run, _ = save_video_of_model(\"Gridworld-v0\", vi.get_action)\n",
        "render_value_map_with_action(env, vi.V, vi.get_action)\n",
        "HTML(play_video(saved_run))\n",
        "trajs = collect_traj(env, policy=vi.get_action, num_episodes=100)\n",
        "plot_trajs(env, trajs)"
      ],
      "metadata": {
        "id": "uYaBQzB09Dm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also plot the expected returns and errors with respect to the number of iterations until convergence:"
      ],
      "metadata": {
        "id": "VvmtJXgi9luH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot error and mean episodic rewards per iterations\n",
        "def plot_reward_error_graphs(ep_rews, errors):\n",
        "    ax = plt.subplot(2,1,1)\n",
        "    ax.plot(ep_rews)\n",
        "    bx = plt.subplot(2,1,2)\n",
        "    bx.plot(errors)\n",
        "    ax.set_ylabel('Mean episode rewards')\n",
        "    ax.ticklabel_format(axis='x', style='sci', scilimits=(0,0))\n",
        "    bx.set_xlabel('Number of iterations')\n",
        "    bx.set_ylabel('Errors')\n",
        "    labels = [item.get_text() for item in ax.get_xticklabels()]\n",
        "    empty_string_labels = ['']*len(labels)\n",
        "    ax.set_xticklabels(empty_string_labels)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_reward_error_graphs(ep_rews, errors)"
      ],
      "metadata": {
        "id": "9nSnT23o9Lk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (2.2) Comparison under different transition models\n",
        "\n",
        "Here, you need to vary $\\epsilon$. Get the necessary results when $\\epsilon=0.1$:"
      ],
      "metadata": {
        "id": "g9c9LZNnAnwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################################\n",
        "##################   DO NOT MODIFY THIS CELL   ####################\n",
        "###\n",
        "#if 'Gridworld-v0' in gym.envs.registration.registry.env_specs:\n",
        "#    del gym.envs.registration.registry.env_specs['Gridworld-v0']\n",
        "register(\n",
        "    id='Gridworld-v0',\n",
        "    entry_point=GridEnv,\n",
        "    max_episode_steps=30,\n",
        "    reward_threshold=100,\n",
        "    kwargs={'epsilon':0.1, 'size':[8, 10], \n",
        "            'obstacle':[15,18,26,27,37,47,50,51,59,66,54],\n",
        "            'trap':[34,35,36,77]}  \n",
        ")                                            \n",
        "env = gym.make(\"Gridworld-v0\")\n",
        "\n",
        "vi = ValueIteration(env, discount_factor=0.9)\n",
        "ep_rews, errors2 = vi.value_iteration()\n",
        "###################################################################\n",
        "\n",
        "###################################################################\n",
        "#####################   PLACE YOUR CODE HERE   ####################\n",
        "###                                                             ###\n",
        "### 1. plot the expected return                                 ###\n",
        "### 2. render the value map overlaying the best action          ###\n",
        "### 3. Plot the distribution of trajectories                    ###\n",
        "\n",
        "# Plot error and mean episodic rewards per iterations\n",
        "\n",
        "\n",
        "# Visualize the policy with the value map and trajectories plot\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###################################################################"
      ],
      "metadata": {
        "id": "GQMIJi_SAf-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the necessary results when $\\epsilon=0.4$:"
      ],
      "metadata": {
        "id": "MxpQTJ3l_ZLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################################\n",
        "##################   DO NOT MODIFY THIS CELL   ####################\n",
        "###\n",
        "#if 'Gridworld-v0' in gym.envs.registration.registry.env_specs:\n",
        "#    del gym.envs.registration.registry.env_specs['Gridworld-v0']\n",
        "register(\n",
        "    id='Gridworld-v0',\n",
        "    entry_point=GridEnv,\n",
        "    max_episode_steps=30,\n",
        "    reward_threshold=100,\n",
        "    kwargs={'epsilon':0.4, 'size':[8, 10], \n",
        "            'obstacle':[15,18,26,27,37,47,50,51,59,66,54],\n",
        "            'trap':[34,35,36,77]}  \n",
        ")                                            \n",
        "env = gym.make(\"Gridworld-v0\")\n",
        "\n",
        "vi = ValueIteration(env, discount_factor=0.9)\n",
        "ep_rews, errors = vi.value_iteration()\n",
        "###################################################################\n",
        "\n",
        "###################################################################\n",
        "#####################   PLACE YOUR CODE HERE   ####################\n",
        "###                                                             ###\n",
        "### 1. plot the expected return                                 ###\n",
        "### 2. render the value map overlaying the best action          ###\n",
        "### 3. Plot the distribution of trajectories                    ###\n",
        "\n",
        "# Plot error and mean episodic rewards per iterations\n",
        "\n",
        "\n",
        "# Visualize the policy with the video and trajectories plot\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###################################################################"
      ],
      "metadata": {
        "id": "otQ4swav_YjV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}